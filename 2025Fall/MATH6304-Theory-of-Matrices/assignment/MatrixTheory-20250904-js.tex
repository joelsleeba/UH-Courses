% initial settings
\documentclass[12pt]{article}
\usepackage{geometry,amsthm,amsmath,amssymb, graphicx, natbib, float, enumerate}
% \usepackage{latexml}
\geometry{margin=1in}
\renewcommand{\familydefault}{cmss}
\usepackage{hyperref}
%\usepackage{charter}
\restylefloat{table}
\restylefloat{figure}

%%%%%%%%%%%% MODIFY LECTURE DATE AND AUTHOR %%%%%%%%%%%%%%%%%

\newcommand\lecdat{September 4, 2025} % INSERT LECTURE DATE HERE

\newcommand\notesby{Joel Sleeba}% INSERT NOTE TAKER HERE

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\date{} % not needed
\author{} % not needed

%%%%%%%%%%% SOME MACROS BELOW %%%%%%%%%%%%%%%%%%%%%%%%%%

\swapnumbers
\newtheorem{thm}{Theorem}[section]
\newtheorem{claim}[thm]{Claim}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{conclusion}{Conclusion}

\newtheorem{lemma}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\theoremstyle{remark}
\newtheorem{rem}[thm]{Remark}
\newtheorem{remarks}[thm]{Remarks}
%\newtheorem{rem}[thm]{Remark}
\newtheorem{ex}[thm]{Example}
\newtheorem{exc}[thm]{Exercise}
%\newtheorem{fact}[thm]{Fact}
\newtheorem{facts}[thm]{Facts}
\newtheorem{prob}[thm]{Problem}
\newtheorem{question}[thm]{Question}
\newtheorem{answer}[thm]{Answer}
\newtheorem{conj}[thm]{Conjecture}

\renewcommand{\thethm}{\thesubsection.\arabic{thm}}

\newcommand{\bb}[1]{\mathbb{#1}}
\newcommand{\cl}[1]{\mathcal{#1}}
\newcommand{\ff}[1]{\mathfrak{#1}}

\newcommand{\norm}[1]{\|#1\|}
\newcommand{\abs}[1]{|#1|}
\def\eps{\epsilon}
\def\del{\delta}

\def\Sn{\mathbb S^n}
\def\Snm1{\mathbb S^{n-1}}

\def\R{\mathbb R}
\def\Rn{\mathbb R^n}

% hyprlink settings
\hypersetup{
  pdfauthor={\notesby},
  % pdftitle={Matrix Theory, Math6304\\
  % Lecture Notes from \lecdat\\[0.1cm] \small taken by \notesby},
  pdfsubject={Matrix Theory, Math6304, Lecture Notes},
  pdfkeywords={Math 6304, Matrix Theory},
  pdfproducer={LaTeX},
  pdfcreator={pdflatex}
}

%%%%%%%%%%% PUT YOUR MACROS HERE %%%%%%%%%%%%%%%%%%%%%%%%

\newcommand\mynewcommand{use often}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\title{Matrix Theory \linebreak
  Lecture Notes
from \lecdat}
\author{taken by \notesby}
\maketitle

\normalsize

\setcounter{section}{1}
\setcounter{subsection}{7}
\setcounter{thm}{28}

\subsection*{Warm Up}

Assume we know that $A \in M_n(\mathbb{C})$ is diagonalizable. Let
$p_0, p_1 , p_2 , \ldots , p_n \in \mathbb{C}$ and consider
\begin{align*}
  B: = P(A) = p_0 I + p_1 A + p_2A^2 + \ldots + p_nA^n
\end{align*}
Is $B$ diagonalizable?
\begin{answer}
  Yes. Let $S \in M_n(\mathbb{C})$ be invertible such that $A =
  S^{-1}DS$ for a diagonal matrix $D \in M_n(\mathbb{C})$. Then $A^n =
  S^{-1}D^n S$ and
  \begin{align*}
    B &= p_0 I  + p_1S^{-1}DS + p_2S^{-1}D^2S + \ldots + p_nS^{-1}D^nS \\
    &= S^{-1} (p_0 I  + p_1D + p_2D^2 + \ldots + p_nD^n)S \\
    &= S^{-1}P(D)S
  \end{align*}
  Since $D$ is a diagonal matrix and the product of diagonal matrices
  are diagonal, $D^n$ is also diagonal. Then $P(D) = p_0 I  + p_1D +
  p_2D^2 + \ldots + p_nD^n$ will also be a diagonal matrix. Hence we
  get that $B$ is diagonalizable.

  In fact we get more, we get that $B$ is diagonalizable by the same
  $S \in M_n(\mathbb{C})$ which diagonalized $A$. In this lecture we
  will be investigating the conditions on $B$ to be diagonalized by
  the same matrix $S$ which diagonalized $A$.
\end{answer}

\begin{rem}[Easter egg]
  If $A, B \in M_n(\mathbb{C})$ are diagonalizable by the same $S$ as
  in the example before, is there a polynomial $P \in \mathbb{C}[x]$
  such that $B = P(A)$?
\end{rem}
\begin{answer}
  \textit{(Hint)} No. Let $A =
  \begin{bmatrix}%{c c}
    1 & 0 \\
    0 & 0
  \end{bmatrix}$ and $B =
  \begin{bmatrix}%{c c}
    0 & 0 \\
    0 & 1
  \end{bmatrix}$. Find other examples.
\end{answer}

\subsection{Simultaneous diagonalization}

\begin{defn}
  Let $A, B \in M_n(\mathbb{C})$ be diagonalizable. We say that $A$
  and $B$ are simultaneously diagonalizable if there exists an
  invertible matrix $S \in M_n(\mathbb{C})$ such that $A =
  S^{-1}D_AS$ and $B = S^{-1}D_BS$, where $D_A, D_B \in
  M_n(\mathbb{C})$ are diagonal matrices.
\end{defn}

\begin{thm}
  Let $A, B$ be diagonalizable. Then $AB = BA$ if and only if they
  are simultaneously diagonalizable by the same $S$.
\end{thm}
\begin{proof}
  Let $D_A = S^{-1} A S$, and $B^\prime = S^{-1} B S$, where $D_A$ is
  a diagonal matrix. Without loss of generality, assume that common
  eigenvalues appear together in $D_A$. That is, if $\lambda_1 ,
  \lambda_2 , \ldots , \lambda_k$ are distinct eigenvalues of $D_A$,
  we are assuming that
  \begin{align*}
    D_A =
    \begin{bmatrix}%{c c c c c c c c c c}
      \lambda_1&  &  &  &  &  &  &  &  &  \\
      & \lambda_1 &  &  &  &  &  &  &  &  \\
      &  & \ddots &  &  &  &  &  &  &  \\
      &  &  & \lambda_1 &  &  &  &  &  &  \\
      &  &  &  & \lambda_2  &  &  &  &  &  \\
      &  &  &  &  & \ddots  &  &  &  &  \\
      &  &  &  &  &  & \lambda_2  &  &  &  \\
      &  &  &  &  &  &  & \lambda_3 &  &  \\
      &  &  &  &  &  &  &  &  \ddots &  \\
      &  &  &  &  &  &  &  &  & \lambda_k \\
    \end{bmatrix}
  \end{align*}
  If not, choose $S$ with required
  additional permutations of the rows.

  Assuming $AB = BA$, we get
  \begin{align*}
    D_AB^\prime &= S^{-1} ASS^{-1}BS \\
    &= S^{-1}ABS \\
    &= S^{-1}BAS \\
    &= S^{-1}BSS^{-1}AS \\
    &= B^\prime D_A
  \end{align*}
  If $B^\prime = [b_{i, j}^\prime]_{i, j = 1}^n$, then by $D_A
  B^\prime = B^\prime D_A$, from the diagonal structure of $D_A$, we get
  \begin{align*}
    \tilde{\lambda}_ib_{i, j}^\prime = b_{i, j}^\prime \tilde{\lambda}_j
  \end{align*}
  where $\tilde{\lambda}_i$ is the $i$-th diagonal entry on $D_A$.
  So, we have
  \begin{align*}
    (\tilde{ \lambda}_i - \tilde{\lambda}_j) b_{i, j}^\prime = 0
  \end{align*}
  which shows that if $\tilde{ \lambda}_i \neq \tilde{\lambda}_j$,
  then $b_{i, j}^\prime = 0$. Thus we get that
  \begin{align*}
    B^\prime =
    \begin{bmatrix}%{c c c c}
      B_1^\prime &  &  & \\
      & B_2^\prime &   & \\
      &  &  & \ddots  & \\
      &  & &   & B_r^\prime \\
    \end{bmatrix}
  \end{align*}

  Since $B$ is diagonalizable, by the definition of $B^\prime$ it
  follows that $B^\prime$ is diagonalizable. We claim that each
  $B_r^\prime$ themselves are diagonalizable. Considering $B^\prime$
  as a linear map $\mathbb{C} \to \mathbb{C}^n$, from the block
  structure of $B^\prime$ we see that there are subspaces $W_i
  \subset \mathbb{C}^n$ such that
  \begin{align*}
    \mathbb{C}^n = \bigoplus_{i = 1}^r W_i
  \end{align*}
  and $B^\prime(W_i) \subset W_i$. Moreover observe that $B_i^\prime$
  is the matrix representation of the linear map $B^\prime$
  restricted to $W_i$. Since $B^\prime$ is diagonalizable, by our
  characterization there is a basis of $\mathbb{C}^n$ consisting of
  eigenvectors of $B^\prime$. Since $B_i^\prime$ is the restriction
  of $B^\prime$ to $W_i$, eigenvectors of $B^\prime$ which are in the
  subspace $W_i$ are eigenvectors of $B_i^\prime$ themselves. And
  they form a basis for $W_i$ as $\mathbb{C}^n$ is the direct
  sum of $W_i$s. Thus from our characterization, we get that each
  $B_i^\prime$ is diagonalizable.

  Taking matrices $T_1 , T_2 , \ldots , T_r$ that
  diagonalize $B^\prime_1 ,  B^\prime_2 , \ldots , B^\prime_r$
  respectively, let
  \begin{align*}
    T =
    \begin{bmatrix}%{c c c c}
      T_1&  &  &  \\
      & T_2 &  &  \\
      &  & \ddots &  \\
      &  &  & T_r \\
    \end{bmatrix}
  \end{align*}
  Then,
  \begin{align*}
    T^{-1} B^\prime T =
    \begin{bmatrix}
      T_1^{-1}B_1^\prime T_1 &  &  & \\
      & T_2^{-1}B_2^\prime T_2 &   & \\
      &  &  & \ddots  & \\
      &  & &   & T_r^{-1} B_r^\prime T_r \\
    \end{bmatrix} =
    \begin{bmatrix}%{c c c c}
      D_1^\prime&  &  &  \\
      &  D_2^\prime&  &  \\
      &  &  \ddots&  \\
      &  &  & D_r^\prime \\
    \end{bmatrix}
  \end{align*} where each $D_i^\prime$ is a diagonal block.
  Also,
  \begin{align*}
    T^{-1} D_A T =
    \begin{bmatrix}
      T_1^{-1}\lambda_1 I T_1 &  &  & \\
      & T_2^{-1}\lambda_2 IT_2 &   & \\
      &  &  & \ddots  & \\
      &  & &   & T_r^{-1} \lambda_r I T_r \\
    \end{bmatrix} = D_A
  \end{align*}
  Thus for $Q = ST$, we get that $Q^{-1}AQ =  T^{-1} S^{-1} A S T = D_A$,
  and $Q^{-1}BQ = T^{-1} S^{-1}B ST = D_B $ are both diagonal,
  proving that $A$ and $B$ are simultaneously diagonalizable.

  Conversely, if $A$ and $B$ are diagonalizable by the same $S$, we have $A
  = SD_AS^{-1}$ and $B = S^{-1}D_BS$. Then
  \begin{align*}
    AB &= S^{-1}D_ASS^{-1}D_BS \\
    &= S^{-1}D_AD_B S \\
    &= S^{-1}D_BD_AS \\
    &=S^{-1}D_BSS^{-1}D_AS \\
    &= BA
  \end{align*}
  And thus we are done.
\end{proof}

Next, we consider simultaneous diagonalization for a family of matrices.
\begin{defn}
  A family $F \subset M_n$ is a commuting family if for each $A, B
  \in F$, $AB = BA$.
\end{defn}

\begin{defn}
  A subspace $W \subset \mathbb{C}^n$ is called an $A$-invariant
  subspace for some $A \in M_n$, if $Aw \in W$ for all $w \in W$. If
  $F \subset M_n$, then $W$ is called $F$-invariant if for each $A
  \in F$, $W$ is $A$-invariant.
\end{defn}

\begin{lemma}
  If $W \subset \mathbb{C}^n$ is $A$-invariant for some $A \in M_n$,
  and suppose that $\textrm{dim}(W) \ge 1$, then there is an $x \in W
  \setminus \{  \textbf{0} \}$ such that $Ax = \lambda x$.
\end{lemma}
\begin{proof}
  Since the subspace $W$ is $A$ invariant, $A$ as a linear
  transformation  restricted to $W$, $A|_W: W \to W$ has a matrix
  representation $B \in M_r$, where $r < n$. $B$ has an
  eigenvector since it has at least one eigenvalue $\lambda$ as the
  characteristic polynomial $p_B(x)$ decomposes into linear factors
  by the fundamental theorem of algebra. Let $x$ be the corresponding
  eigenvector in $W$ such that $Bx = \lambda x$. Now considering $B$
  as the restriction of $A$ to $W$, we see that $Ax = \lambda x$.
  Hence we are done.
\end{proof}

\begin{lemma}
  If $F \subset M_n$ is a commuting family, then there exists an $x
  \in \mathbb{C}^n$ such that for each $A \in F$, $Ax = \lambda_A x$.
\end{lemma}
\begin{proof}
  Choose $W$ to be an $F$-invariant subspace of minimum, non-zero
  dimension. Existence of $W$ is guaranteed since $\mathbb{C}^n$ is
  an $F$-invariant subspace of non-zero dimension.

  Next, we show that any $ x \in W \setminus \{ \textbf{0} \}$ is an
  eigenvector for each $ A \in \mathbb{F}$. Assume this is not true.
  Then there is  a $ y\in W \setminus \{ \textbf{0} \}$, and an $ A
  \in F$, such that $Ay \not\in \mathbb{C}y$. Since $W$ is
  $A$-invariant by the setup, by previous lemma, we get that there is
  a $x \in W \setminus \{ \textbf{0} \}$ such that $Ax = \lambda_x x$
  for some $\lambda_x \in \mathbb{C}$.

  Let $ W_0 := \{ z \in W \ : \ Az = \lambda_x z \}$. Since $y \notin
  W_0$, we get that $W_0 \subsetneq W$. But for any $B \in F$, by the
  invariance of $W$, $Bx \in W$. Then for $u \in W_0$,
  \begin{align*}
    A(Bu) = B(Au) =  \lambda_x Bu
  \end{align*}
  and since $Bu \in W$ and it satisfies the description of the set
  $W_0$, we observe $Bu \in W_0$. Thus $B$ maps $W_0$ to $W_0$. Since
  $ B \in F$ was arbitrary this shows that $W_0$ is $F$-invariant.
  Hence have derived a contradiction with the
  minimality of $W$, proving our statement.
\end{proof}

\begin{rem}
  This implies that commuting families have at least one common eigenvector
\end{rem}

\begin{defn}
  A simultaneously diagonalizable family is a family $F \subset M_n$
  such that there exists $S \in M_n$ for which $S^{-1} A S$ is
  diagonal for each $A \in F$
\end{defn}

\begin{thm}
  Let $F \subset M_n$ be a family of diagonalizable matrices, then
  $F$ is a commuting family if and only if it is simultaneously diagonalizable.
\end{thm}
We will prove this in the next lecture.

\end{document}
